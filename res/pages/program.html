<div class='top100'></div>
<h1 class='pageTitle'> Program </h1>

<div class='content'>
  
  <button onclick='dayClicked(0)' class='card'>
    <h2>Day 1</h2>
    <h3>Thu, October 10th</h3>
  </button>
  <button onclick='dayClicked(1)' class='card'>
    <h2>Day 2</h2>
    <h3>Fri, October 11th</h3>
  </button>
  <button onclick='dayClicked(2)' class='card'>
    <h2>Day 3</h2>
    <h3>Sat, October 12th</h3>
  </button>
  
  <div id="accordion" class="">
    <h2 class='tcenter fwnormal' id='d1wp'>Day 1</h2>
    <h3 class='tcenter fwnormal'>Thu, October 10th</h3>
    <h4 class='tcenter legend' style='font-size:90%'>
      <span class='greenDot'></span> Performance
      <span class='yellowDot'></span> Breaks
      <span class='blueDot'></span> Presention
    </h4>
			<div class='pcard'>
        <label><span class='blueDot'></span><span class='clock'></span>12:00-16:30</label>
				<div class='article'>
            REGISTRATION OPEN
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>

      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="check-1" />
				<label for="check-1"><span class='blueDot'></span><span class='clock'></span>12:00-12:30</label>
          <h3> Retrieving Human Traits From Gesture In Sign Language : The Example Of Gestural Identity </h3>
          <h4>Félix Bigand, Elise Prigent and Annelies Braffort </h4>
          <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
  ABSTRACT. Virtual signers (or signing avatars) play an important role in the accesibility of information in sign languages. They have been developed notably for their capability to anonymize the signer's appearance and to enable dynamic or interactive scenarios. Recording real movements thanks to motion capture provides human-like, realistic and comprehensible signing animations. However, such accurate systems may also convey extralinguistic information such as identity, gender or emotional state. In the present work, we want to address the problem of gestural identity in the context of animated agents in French Sign Language (LSF). On the one hand, person identification from signing motion is assessed through psychophysical experiments, using point-light displays. On the other hand, a computational framework is developed for the analysis of LSF motion in order to investigate which features are critical for identification. For some applications, determining these movement parameters will enable controlling the gestural human traits of virtual signers.
				</article>
			</div>

      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="check-3" />
				<label for="check-3"><span class='blueDot'></span><span class='clock'></span>12:30-13:00</label>
        <h3>Tracing from Sound to Movement with Mixture Density Recurrent Neural Networks </h3>
        <h4>	Benedikte Wallace, Charles P. Martin and Kristan Nymoen</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
ABSTRACT. In this work, we present a method for generating 3D tracings of perceived sound features, sound-tracings, using a Mixture-Density Recurrent Neural Net (MDRNN). By training the model on a data set of single point sound-tracings and the sound features extracted from short sounds (2 to 4 seconds) the model learns to generate novel tracings using multi-modal input. This is part of an ongoing research effort to examine the complex correlations between sound and movement and the possibility of modelling these relationships using deep learning techniques.
				</article>
			</div>

      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="check-4" />
				<label for="check-4"><span class='blueDot'></span><span class='clock'></span>13:00-13:30</label>
        <h3>Towards AI-Enhanced Ballet Learning</h3>
        <h4>	Milka Trajkova </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. Since its codified genesis in the 18th century, ballet training has largely been unchanged: it relies on the word of mouth expertise passed down generation to generation and in tools that do not adequately support both dancers and teachers. Moreover, top-tier training is only found in a few locations around the world and comes at an exceptional price. In this context, artificial intelligence (AI)-based video tools might represent an affordable and non-invasive alternative: it would allow dancers and teachers to self-assess as well as enable skilled dance teachers to connect with a wider audience. In my research, I study how to design and evaluate AI-based tools to improve ballet performance for dancers and teachers.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="csdcv" />
				<label for="csdcv"><span class='blueDot'></span><span class='clock'></span>13:30-14:00</label>
        <h3> Adipose Tissue: Localizeable Relations and the Fat Body </h3>
        <h4> Will Hallett </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. Much of the scholarship and discourse surrounding networks, ‘making’, and liberation deal intermittently with the notion, or problem, of locality. This mirrors a broader political debate concerning the problematic between global politics and local or hyper-local notions of resistance, action, conservation, or community. Simultaneously, celebrity communist Slavoj Zizek is applauded by amphitheater crowds for his sustained faith in a central state structure, and hackers and technologists flock to decentralized network structures, ‘radical networks’, and peer-to-peer hardware . Frederick Jameson’s work on cognitive mapping quite explicitly identifies what might be a central issue; namely that we have no conception today of the relation of the local to the global, most stunningly evidenced by our relation to the planet and to its climate. My current work and practice tries to approach this problem as a philosophical one, focused on the super-locality of the fat body as a particular confluence of morbidity, dissociation, and aberrance and a useful case study for a disruption of popular understanding of bodies and transformation through a queering of extensive space and a naive proposition towards the topological localization of affect.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="f4" />
				<label for="f4"><span class='blueDot'></span><span class='clock'></span>14:00-14:30</label>
        <h3> Curious Creatures: a living virtual research-creation lab</h3>
        <h4> Sarah Vollmer </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. The Curious Creatures project is an exploratory Research-Creation journey. Here, the practice of Digital Media in a Virtual Reality medium is developed as an ongoing and mutable processual methodology. Sensorial engagement and embodiment practices are explored through practical exposure and conceptual immersion. Interactions of human and computer (as both agents of design and agents of use during the creation process) mirror intellectual and emotional decisions faced throughout the ongoing construction process. Parallels are drawn to existing art, conceptual frameworks, engineering practices, and technology that inspire this curiosity driven exploration.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="df" />
				<label for="df"><span class='blueDot'></span><span class='clock'></span>14:30-:15:00</label>
        <h3> Eco-Somatic Performance Worldmaking </h3>
        <h4> Timothy Wood </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. Caring for and empowering the self is primary in creating space for healthy relationship with other. In the context of the technologically mediated other, how can one maintain solid ground in our embodied selves while creating new spaces and practices for meeting other? How can technological utilization reflect the beauty and supporting qualities that can be found in the otherness of the natural world? What practices and stories do we learn from listening to our senses, our bodies, and our imagination in relationship to otherness, both in technological extended environments and the natural world, and how can these learned practices and stories be translated and transformed between the personal, the collective, the virtual, and the real?

My practice based research is a search in the space connecting bodies to the immersive qualities and healing potential of nature, virtual reality, creative expression, and the imagination. If the criterion is a somaesthetic one, where learning, discovering, sensitizing, and honing the body's connection to seeing and shaping itself is a priority, how can immersive and interactive virtual worlds support and reflect this somaesthetic intention? What supports connection to self, in how we approach technology? What can we learn by clarifying our intention, focusing our attention, delineating clear boundaries in this relationship. If the criterion is an ecological one, how can we acknowledge the mystery and the strangeness of other? How can we experience the interconnection of living and non-living entities in curiosity and openness? How does the ecological inform the technical, and how do virtual worlds imbued with ecologocial thought, self-organizing systems, and artificial life change the somaesthetic, improvisational, creative potential in relationship to moving feeling bodies?

In this short paper, I will briefly discuss where this practice based research stems from, and what theories, practices, and related works guide its search, as well as initial performance experiments and future research directions.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="zda" />
				<label for="zda"><span class='greenDot'></span><span class='clock'></span>12:30-14:30</label>
        <h3> Session 2A: Installation  -- Qualia </h3>
        <h4> Lori Brungard</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. Qualia is a participatory event that engages MOCO conference attendees in both subtle and overt ways, using simple technology that is readily at hand – their own phones. Qualia is a term that refers to the internal and subjective component of sense perceptions. The overall goal of this work is to forge physical, psychological, aural, and visual connections through embodied means, via a contemporary update on the postmodern task based movement score.

I propose that the work take place during the opening reception for the conference, when people register, and expect to interact with one another anyway. At registration they are invited to text the word “Affect” to an SMS short code on their phones, from which they will receive text message prompts for movement tasks throughout the evening. “Affect” can be read as both a noun and a verb. As such, it implies both an emotional state and the fact that our behavior can influence others. Initially, participants will be prompted to do simple, individual tasks, such as “Sit on the couch,” that they might do anyway, tailored to the specific environment based on consultation with the conference organizers. As they become more comfortable, they will be asked to interact: “Share your food with someone,” or “Introduce yourself with your partner’s last name.” As the participants move around the space, they make self-determined choices to comply with or ignore the prompts they receive. They listen, follow, lead, move, or recline—becoming part of the imagery of the work in ways that collude, conflict, and conjoin with one another. As the evening continues, the prompts will invite the participants to interact more actively with each other, with instructions such as “Gesticulate,” or “Mirror someone.” Some prompts are abstract, like “Reject convention”. While not physically possible, these prompts are intended to create an imaginal effect, a form of qualia.

By spontaneously interacting with the environment and one another, the participants generate their own phenomenological experiences. Thus, this work employs the concept of qualia as embodied experience to expand our understanding of one-on-one relationship to a broader cultural one. Varied interpretations of meaning in Qualia evoke questions of what is real, and how our perceptions of reality are influenced by others’ views. The intention is to both reveal these lines of difference, and also to sew them together, creating interstitial, communal bonds in a way that still acknowledges individual difference. Movement happens in the in-between, creating interstitial connections from one place and one person to another. It is the leap that must be made between two entities that creates connection. By prioritizing movement as the means of communication, the participants can foster intangible yet viscerally remembered connections as both spectators and participants.

				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="aq" />
				<label for="aq"><span class='greenDot'></span><span
            class='clock'></span>12:30-16:30</label>
        <h3> Session 2B: com//ense </h3>
        <h4> Performance - Installation </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, Gallery</a></h4>
				<article>
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
   <!--
				<input type="checkbox" id="" />
    -->
				<label for=""><span class='blueDot'></span><span
            class='clock'></span>15:00-16:15</label>
          <h3> Session 3: Keynote</h3>
          <h4> Kristina Höök</h4>
          <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B, Gallery</a></h4>
          <h4><a href='javascript:gotoA(0)'><span class='bio'></span> Presenter Bio</a></h4>
   <!--
          <article>
          </article>
    -->
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="a10" />
				<label for="a10"><span class='blueDot'></span><span class='clock'></span>16:30-18:00</label>
        <h3> Session 4A: Workshop -- Deconstructing Gesture: Investigating Embodied Motion</h3>
        <h4> Kristin Carlson and Greg Corness </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. This workshop aims to explore and discuss the term "gesture'' in order to better understand movement artist and scholar`s nuanced approach to dynamic quality, as an early step towards developing more humanistic computational models.
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="r4" />
				<label for="r4"><span class='blueDot'></span><span class='clock'></span>16:30-18:00</label>
        <h3> Session 4B: Workshop -- Modeling Dance History and Embodied Data: Some Approaches to Katherine Dunham’s Movement on the Move</h3>
        <h4>  Kate Elswit, Antonio Jimenez-Mavillard and Harmony Bench </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. This lecture-demonstration and participatory workshop offers hands-on exploration of the data visualizations generated for our research project, Dunham’s Data: Katherine Dunham and Digital Methods for Dance Historical Inquiry. Dunham’s Data is a three-year project devoted to the kinds of questions and problems that make the analysis and visualization of data meaningful for the field of dance studies. We work with mid-century African American choreographer Katherine Dunham as a case study to bring a historical perspective to thinking about how dance moves both across geographical locations as well as across cultural, artistic, and financial networks. We have thus far manually curated datasets representing 10 years of her performing career from undigitized archival documents, with the goal of representing over 30 years of her touring and travel. We apply computational analytic approaches to this collated data, including spatial and network analysis, among other methods. We are also working with other visualization strategies in order to consider how to represent embodiment digitally, without reducing lived experience to data. Our commitments are to feminist and antiracist research in critical data studies, and we follow imperatives to “bring back the bodies” in digital research (D’Ignazio and Klein 2019) in a literal way. Our training as dance scholars enables greater access to the personal, embodied experiences that both underpin and haunt the data we have collected from Dunham’s archives.

As MOCO and DSA collaborate on this year’s conference, we are excited to use this opportunity to share the ways in which we have approached the challenges of representing bodily experience within computational models of dance history, shaped by approaches to embodiment from dance, critical race theory, and digital cultures. We propose a 60-minute practice work session, in which we will introduce the overall project, and then highlight key aspects of work done to date. This includes showcasing and beta-testing several visualizations of Dunham’s transnational labor, from the ways in which patterns of travel correlate with financial support, to the corporeal wear and tear produced by her “movement on the move” (Bench and Elswit 2016). We will include visualizations of interactive timelines of travel that will be contextualized within the experiential work of spatial history, as well as static visualizations that offer insight into multiple simultaneous states of corporeally-situated wellbeing. In addition, we will show exploratory work around the ever-changing patterns of participation that define “the company” and the ways in which this relates to choreographic repertoire. We will invite participants in the session to interact with, explore, and comment on these visualizations. Building on this participation, we are particularly interested in cultivating a discussion around the ways that movement languages from dance and computing can shape the trajectory and development of scalable digital analytic methods for dance history, at the same time that historical studies may push computational modeling and representation of movement in new directions as well.
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="df2" />
				<label for="df2"><span class='blueDot'></span><span class='clock'></span>16:30-18:00</label>
        <h3> Session 4C: Workshop -- Dance Notation as a Means of Preserving Embodied Knowledge</h3>
        <h4> Rommie Stalnaker and Susan Wiesner </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. MOCO workshop (90 minute) proposal: Dance Notation as a Means of Preserving Embodied Knowledge

This workshop proposal steps outside of the digital in order to return to the digital. Recent experiences in DH circles have supported our belief that those in DH are becoming more interested in the possibilities for research offered by the performing arts, humanities writ large. That interest is often due to a greater desire to understand embodied knowledge, how it is transferred, preserved, and developed through, and in support of, digital projects. In response to this interest, one aspect of our research considers embodied knowledge production/preservation, and transfer, while it addresses a practice versus technology ‘gap’. And although we see a myriad of possibilities for theoretical and applied research emerging out of this -- many of which we are engaged in (e.g. developing pedagogical tools for dance, automated annotation, controlled vocabularies and metadata generation, building ontologies, etc) -- we repeatedly find ourselves focusing on the lack of a way to share data generated by our individual areas of expertise through computational means. For us, what is missing is a computationally tractable notation system that supports our work across multiple art forms: dance, music, animation, scripted and improvisational theatre, etc.

So, in order to insert a notational substrate into the picture of multimodal, embodied performance studies, we have begun working with movement-based notational forms and turned first to work conducted by Irmgard Bartenieff and Albrecht Knust where they transcribed Beauchamps/Feuillet Notation into Labanotation. In addition, as we move forward in our effort to develop computationally tractable notation and annotation, we extend our study of movement notation forms to include Motif Writing developed by Valerie Preston-Dunlop, Anne Hutchinson Guest, Bartenieff, and others.

Yet, as we discovered when working across disciplines, few researchers outside of dance (and admittedly within dance) know Labanotation and/or Motif, much less Beauchamp/Feuillet notation. Therefore, we propose a 90-minute workshop on movement observation and notation systems that provide a means of transference and preservation of embodied knowledge across multiple art forms. A second realization for the need for this workshop was sparked at the 2018 Music Encoding Initiative (MEI) conference, where attendees were completely unaware of Beauchamps/Feuillet notation and when exposed to it became quite excited by the prospect of incorporating the connection between music and movement notation systems. Following that response, we plan to expose workshop participants to four notation systems: Beauchamps/Feuillet notation, Labanotation, Motif Writing, and Memory (the latter is more esoteric, but we have created plans for delving into the function of memory).

Open to all, participants from any discipline will have opportunities to read graphic movement notation as well as observe movement and discuss options for notating it. To support an understanding of these systems, we will begin with a brief history of the systems, with an awareness of cultural underpinnings that influenced their development (e.g. for Feuillet: Western court dance, French vs German vs English interpretations, etc). We will then offer an overview of the Laban/Bartenieff Movement System as it pertains to the body and the use of graphic notation to describe and transfer knowledge of movement. As this is about movement specifically (granted, Feuillet does include musical scores and it will be discussed, but is not the focus of this workshop), we will have participants move and explore the relationship between body and graphic notation. As participants dig into movement from the Baroque period, so, too, will they gain awareness of the value of using LBMS and notation in a variety of digital projects. By offering this taste of these graphic systems, we hope to promote meaningful, multi-disciplinary collaboration and build a valuable dialogue across disciplines, as we ask what notation brings to the preservation and transfer of embodied knowledge through the performing arts. We recognize that this is ambitious for a 90-minute workshop, however we will offer a sample of using notation as a means of preserving embodied knowledge.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="is" />
				<label for="is"><span class='blueDot'></span><span class='clock'></span>16:30-18:00</label>
        <h3> Session 4D: Workshop -- Co-constructing Events in Responsive Environments  </h3>
        <h4> Xin Wei Sha, Jessica Rajko, Todd Ingalls, John MacCallum, Teoma Naccarato, Lauren Hayes, Garrett Johnson, Emiddio Vasquez, Brandon Mechtley, Chris Ziegler, Seth Thorn, Connor Rawls, Peter Weisman, Assegid Kidane, Yanjun Lyu and Shomit Barua </h4>
        <h4><a href='javascript:goto("is")'><span class='pin'></span>
            iStage, Matthews Center II Floor</a></h4>
				<article>
          ABSTRACT. A series of workshops, performances and
        installations hosted by Synthesis and AME at the iStage

We present a suite of approaches to how ensembles of people, technical objects, and processes can co-construct events that make ethico-aesthetic sense to the participants. The intents and techniques range widely: from creative uses of gesture-following or vibro-tactile feedback or whole body interaction in performance works (e.g. Naccarato, MacCallum, Hayes, Rajko, Ziegler, Thorn), to using body-borne sensing, and camera / acoustic feature following and realtime media to study the dynamics of rhythm, sense and affect. (e.g. Sha, Ingalls, Johnson, MacCallum, Naccarato, Rajko.)

We are interested in holistic approaches to the heightening of felt, movement-based experience that recognize (1) experience cannot be reduced to any finite schema or data, (2) qualities of experience, being relational, cannot be read from measurements taken at one point, one body, or one instant, (3) that distinctions like subjects and objects, signal and noise, intentional gesture and non-intentional movement may emerge in the course of an event. and do not exist as categories prior to that event, (4) the significance of a sign or movement may lie in its response, and thus cannot be determined solely by its features.

Methodologically we follow minimax design practice: maximum experiential impact for minimum engineering. At the same time, we avail ourselves of state of the art, original methods in signal processing, computational physics, and experimental experiential sciences, where and as appropriate (Mechtley, Sha, Vasquez). Some of the approaches prototype humane versions of non-anthropocentric, ecosystems design of responsive environments that evolve in concert with both contingent, improvised activity as well as design intent.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="q2" />
				<label for="q2"><span class='blueDot'></span><span class='clock'></span>16:30-18:00</label>
        <h3> Session 4E: Workshop -- Moving Between Worlds</h3>
        <h4> </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. In practicing relationship with the earth, the air, the plants, the animals, the present moment, and the aliveness that continuously draws attention, one hones the ability to see and be seen by other. In the context of the technologically mediated other, how can one maintain solid ground in our embodied selves while creating new spaces and practices for meeting other? How can technological utilization reflect the beauty and supporting qualities that can be found in the otherness of the natural world? What practices and stories do we learn from listening to our senses, our bodies, and our imagination in relationship to otherness, both in technological extended environments and the natural world, and how can these learned practices and stories be translated and transformed between the personal, the collective, the virtual, and the real?

In this workshop, we will explore practices in developing improvisational movement scores in relationship to physical space, natural environments, and interactive virtual worlds. No prior movement experience is necessary. We will be playing and moving outside, as well as with an interactive virtual world, and sharing experiences, stories, ideas, and dreams. How do we stay connected to our bodies and prioritize deeply embodied experiences in relationship to technology and virtual realities? How can practices that engage with internal sensation and imagination provide a foundation of self-care in order to encourage the curiosities of playful interaction and improvisation with external otherness?
				</article>
			</div>

   <div class='hr'></div>

			<div class='pcard'>
				<label for=""><span class='yellowDot'></span><span class='clock'></span>18:30-20:30</label>
        <h4><a href='https://g.page/the-normal-restaurant-and-bar?share' target='_blank'><span class='pin'></span> Normal Bar, Graduate Hotel</a></h4>
				<div class='article'>
            Welcome Reception
				</div>
			</div>


      <div class='hr'></div>
      <br><br><br><br>

    <h2 class='tcenter fwnormal' id='d2wp'>Day 2</h2>
    <h3 class='tcenter fwnormal'>Fri, October 11th</h3>
    <h4 class='tcenter legend' style='font-size:90%'>
      <span class='greenDot'></span> Performance
      <span class='yellowDot'></span> Breaks
      <span class='blueDot'></span> Presention
    </h4>
			<div class='pcard'>
        <label><span class='blueDot'></span><span
            class='clock'></span>08:00-11:00</label>
				<div class='article'>
            REGISTRATION OPEN
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d23" />
				<label for="d23"><span class='blueDot'></span><span
            class='clock'></span>09:00-09:25</label>
        <h3> Performance, Art, and Cyber-Interoceptive Systems (PACIS)</h3>
        <h4> Mark-David Hosale, Erika Batdorf, Kate Digby and Alan Macy </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B125 </a></h4>
				<article>
          ABSTRACT. This paper provides a survey of the research-creation activities of the collaboration, Performance, Art, and Cyber-Interoceptive Systems (PACIS). PACIS has been exploring how technology can help us create deeper connections with the world around us, each other, and ourselves by combining bioinformatic sensing technology with physiological awareness techniques found in The Batdorf Technique (TBT). Primary outputs of this research involve the development of new and novel interfaces that integrate complex physiological data in performance and computational art contexts. The focus of this endeavour is on the sharing of knowledge and exchange of ideas across disciplines, the traineeship of students and emerging scholars in the technological as well as somatic techniques and the creation of workshops, articles and performances.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d35" />
				<label for="d35"><span class='blueDot'></span><span
            class='clock'></span>09:25-09:50</label>
        <h3> Using Training Technology Probes in Bodystorming for Physical Training</h3>
        <h4> 	
Elena Márquez Segura, Laia Turmo Vidal, Luis Parrilla Bel and Annika Waern</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
ABSTRACT. A promising domain for wearable technology for physical training is as assistive tools to help people access and act upon their proprioceptive and vestibular senses. To design these tools in close relation to a targeted training practice, we propose an embodied design activity using Training Technology Probes (TTPs). These are pieces of technology with a simple interactivity augmenting and exteriorizing cues from those senses. Here, we explore how to design new TTPs, and explore the usefulness of existing TTPs as design material to spur creativity in embodied design ideation methods. We report on an embodied co-creation design workshop to generate new technology ideas for an ongoing technology-supported circus training course for children with motor difficulties. We characterize the resulting design concepts, elaborating on three that were implemented as TTP prototypes, and show their relevance in several physical training domains. We also bring forward a novel form of technology-supported bodystorming, adding to previous bodystorming methods.
				</article>
			</div>
     

      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d26" />
				<label for="d26"><span class='blueDot'></span><span
            class='clock'></span>09:50-:10:15</label>
        <h3>Shifting Spaces: Using Defamiliarization to Design Choreographic Technologies That Support Co-Creation </h3>
        <h4>Kristin Carlson, Sarah Fdili Alaoui, Greg Corness and Thecla Schiphorst </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. Choreography includes much improvisation and situated decision-making. These embodied abilities have inspired recent trends in HCI to design systems for nuanced experiences of movement. While there are many systems that support aspects of choreographic composition such as generating rules for improvisation or stringing together poses to create a dance, this trend suggests there is room for developing interactive technologies that can more deeply support creative procedures from an embodied perspective. We look to defamiliarization as one tactic that can enable new perspectives in the creative process during otherwise familiar experiences. Defamiliarization can be described as an attentional technique designed to engage a human user‘s decision-making capabilities to provoke unfamiliar approaches to their own creativity. We became interested in more deeply analyzing the interaction between mover and choreographic systems when we discovered limitations when attempting to choreograph collaboratively with a variety of existing systems. This led us to look more closely at how defamiliarization has been used in existing human-computer interaction projects, to understand provocative interaction in a different domain. We then applied the same analysis to choreographic technology projects to confirm the creative options enabled by this framework. This paper presents a framework for co-creative systems that proposes analytical components of: Disorientation, Open-Play, Closed-Exploration, and Balanced Creativity. These components focus on design for choreography yet they can be applied to many creative domains. To test our framework, we also present a variety of speculative designs that would engage with a choreographer in the sensory exploration, movement generation, and composition processes.
				</article>
			</div>

      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='yellowDot'></span><span
            class='cup'></span>10:15-10:30</label>
				<div class='article'>
            COFFEE BREAK
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d27" />
				<label for="d27"><span class='blueDot'></span><span
            class='clock'></span>10:30-10:55</label>
        <h3> Gesture-Ink-Sound: Linking Calligraphy Performance with Sound</h3>
        <h4> 	Jan Schacher and Lia Wei</h4>

        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. In calligraphy, a brush stroke is rooted in an inner image, breath and the uninterrupted flow of movement. The same can be said of a bow stroke on a string instrument or a note sounded on a wind instrument. This article documents the encounter between a specific, two-person form of calligraphic performance, movement analysis techniques, and the mapping of brush gestures to sound processes. It shows how, based on data obtained in motion-capture sessions, the link between gesture and sound is established. This enables different models of sound processes, their specific mode of operation, and the understanding of what makes a stroke. Questions and issues arising from this concrete work are collected and a reflective analysis is carried out via a diagrammatic process. A discussion of critical limitations and possible extensions in this configuration concludes the article.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d210" />
				<label for="d210"><span class='blueDot'></span><span
            class='clock'></span>10:55-11:20</label>
        <h3> Bodily Signals Entrainment in the Sound of Music</h3>
        <h4> Vilelmini Kalampratsidou and Elizabeth Torres</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. As a user listens to music, his bodily biorhythms can entrain with the music's rhythms. This work describes a human computer interface used to characterize the evolution of the stochastic signatures of physiological rhythms across the central and the peripheral nervous systems in the presence (or absence) of music. We track the heart, EEG and kinematics' variability under different music-driven conditions to identify the parameter manifold and context with maximal signal to noise ratio as well as to identify regions of maximal and minimal statistical co-dependencies of present events from past events.
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d211" />
				<label for="d211"><span class='blueDot'></span><span
            class='clock'></span>11:20-11:45</label>
        <h3> Instruments of Articulation: Signal Processing in Live Performance</h3>
        <h4> Seth Thorn and Xin Wei Sha</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. Building on the first author’s hybrid/augmented violin practice and the second author’s work with responsive media environments, we build and reflect on collectively-played room- scale instruments that afford the precision and nuance of an individually-played real-time gestural media system. We consider gestural instruments designed for the interplay of action and perception at the sensorimotor level bypassing tokenization of features of activity and sensors. Our gesturally-modulated media instruments are based not on models or a priori schemata but driven by continuous adaptation to contingent activity and state of the event, as well as compositional intent. We think of such performable, expressive systems as instruments of articulation rather than of representation. Our work is motivated by a progression from phenomenological interpretations of individually-played instruments through non- anthropocentric notions of lived experience, to ecosystemic approaches to ensembles of realtime instruments, people and processes concurrently co-articulating an event.
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d212" />
				<label for="d212"><span class='blueDot'></span><span
            class='clock'></span>11:45-12:00</label>
        <h3> The Airborne Instruments nUFO: a Movement Based Musical
          Instrument for Possibility Space Exploration <i>[published
          preprint]</i> </h3>
        <h4> Hannes Hoelzl, Isak Han and Alberto de Campo</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. The Airborne Instruments nUFO (nontrivial/new flying object) is a new movement based Digital Music Instrument designed for maximal motional freedom of the performer. A handheld wireless Interactor digitizes large scale movement via a 9-axis IMU together with a set of 8 touch-sensitive pads for fine motor finger action. The corresponding software, nUFO_App, applies elaborate meta-mapping strategies (called Influx) to the movement data to inform a number of sound processes such that even very simple movements create complex changes in the sound, which frees players from distracting technical concerns, and empowers them to focus on playing by listening and intuitive motion.
The nUFO distills 15 years of research into complex sound synthesis, just-in-time programming, modal control, and meta-control strategies with a physical Interactor, ergonomically designed from scratch for intuitively exploring the possibility spaces of such systems.
				</article>
			</div>



      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='yellowDot'></span><span
            class='cup'></span>12:00-13:30</label>
				<div class='article'>
            LUNCH BREAK
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>

      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='greenDot'></span><span
            class='clock'></span>12:00-13:30</label>
				<div class='article'>
            Session 7A: Poster Session #1, 6 posters
            <br><a href='javascript:goto("sfb")'><span
                  class='pin'></span> Stauffer B, B 123 </a>
				</div>
			</div>


      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='greenDot'></span><span
            class='clock'></span>12:00-13:00</label>
				<div class='article'>
            Session 7B: KHONG KHRO 
            <br><a href='javascript:goto("sfb")'><span
            class='pin'></span> Stauffer B, Gallery </a>
				</div>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d220" />
				<label for="d220"><span class='blueDot'></span><span
            class='clock'></span>12:00-12:30</label>
        <h3> Session 7C: Intimidatrix</h3>
        <h4> Kristina Warren</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, Gallery </a></h4>
				<article>
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
   <!--
				<input type="checkbox" id="" />
    -->
				<label for=""><span class='blueDot'></span><span
            class='clock'></span>13:30-14:45</label>
          <h3> Session 8: Keynote</h3>
          <h4> Maaike Bleeker</h4>
          <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
              Stauffer B, B125</a></h4>
          <h4><a href='javascript:gotoA(1)'><span class='bio'></span> Presenter Bio</a></h4>
   <!--
          <article>
          </article>
    -->
			</div>



      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='yellowDot'></span><span
            class='cup'></span>14:45-15:00</label>
				<div class='article'>
            COFFEE BREAK
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d240" />
				<label for="d240"><span class='blueDot'></span><span
            class='clock'></span>15:00-15:25</label>
        <h3> Toward Expressive Multi-Platform Teleoperation: Laban-Inspired Concurrent Operation of Multiple Joints on the Rethink Robotics Baxter Robot in Static and Dynamic Tasks</h3>
        <h4> Yichen Zhou, Maxwell Asselmeier and Amy LaViers</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. Human motion calls upon embodied strategies, which can be difficult to replicate in teleoperation architectures. This paper presents a teleoperation method that centers around the Space component of Laban Movement Analysis and may improve the dynamic complexity of teleoperation commands, allowing a trained user to command multiple joint angles at one time via a large database of stored poses, which are indexed by Space parameters. In this paper, this method is compared to a benchmark method, utilizing a joint-by-joint manner of control on a Rethink Robotics Baxter with compliant limbs using a Microsoft Xbox controller. Across four tasks with a trained operator, analysis of the number of active joints at a given point in time and time to completion emphasize the utility that comes with the proposed method. In particular, for the two presented static tasks, the average number of joint angles moving at one time improves and completion times reduce for the proposed method. Plots of behavior show additional qualitative differences in operator strategies and resulting motion, which are also discussed. Future work will extend this initial demonstration to more formal trials with multiple operators. This method may help achieve more fluid, continuous, and improvised motion in teleoperation of robots via gamepads as are currently used in disaster response platforms.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d242" />
				<label for="d242"><span class='blueDot'></span><span
            class='clock'></span>15:25-15:50</label>
        <h3> Tonight We Improvise! Real-time tracking for human-robot improvisational dance</h3>
        <h4> Elizabeth Jochum and Jeroen Derks</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. One challenge in robotics is to develop motion planning tools that enable mobile robots to move safely and predictably in public spaces with people. Navigating public spaces requires a high degree of information about context and environment, which can be partly identified by understanding how people move. Motivated by research in dance and robotics, we developed an exploratory study of improvisational movement for dance performance. Dance forms have recognizable styles and specific interaction patterns (turn-taking, tempo changes, etc.) that reveal important information about behavior and context. Following extensive iterations with expert dancers, we developed a sequence of basic motion algorithms based on improvisation exercises to generate three unique, original performances between a robot and human performers trained in various dance styles. We developed a novel method for tracking dancers in real time using inputs to generate choreography for non-anthropomorphic robots. Although the motion algorithms were identical, the individual dancers generated vastly different performances and elicited unexpected motions and choreographies from the robot. We summarize our study and identify some challenges of devising performances between robots and humans, and outline future work to experiment with more advanced algorithms.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d245" />
				<label for="d245"><span class='blueDot'></span><span
            class='clock'></span>15:50-06:05</label>
        <h3> Embodied Intention: Robot Spinal Initiation to Indicate Directionality</h3>
        <h4> Greg Corness and Kristin Carlson </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. This paper explores how a spine-inspired robot can be designed to provide embodied cues to performers, in order to indicate its intention to change direction through its shifting of weight. Our prior work has explored the importance of utilizing body-level cues such as breath and center of gravity to provide more intuitive information about how a robot, media, or physical agent can act in collaboration with human performers. We detail the design and implemented process of the spinal robot and situate this exploration in a body of work on embodied interaction.
				</article>
			</div>

      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='yellowDot'></span><span
            class='cup'></span>06:05-06:15</label>
				<div class='article'>
            COFFEE BREAK
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<label for=""><span class='greenDot'></span><span class='clock'></span>16:15-17:45</label>
        <h3> PANEL -- Generative tension in cross-disciplinary collaboration: Call for provocations and panelists at MOCO 2019</h3>
        <h4> John MacCallum, Teoma Naccarato and Jessica Rajko</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. We pose a question of great significance to the MOCO community, that is: what aspects of your practice/research are invisible to your collaborators? We seek responses from individuals and teams engaged in cross-disciplinary research and collaboration spanning the broad array of practices implicated in the field of ‘movement and computing’. Responses may be in the form of a succinct online provocation (in image, sound, writing, video, or other), and may be submitted independently or collectively. In addition, a group of panelists will be invited to convene for a public discussion at MOCO 2019, drawing on the body of provocations to address the notion of generative tension in cross-disciplinary collaboration. The intention of this panel is to draw out and mobilize critical differences between the motives and methods of various disciplinary communities as a source of mutual inspiration and innovation.
				</article>
			</div>




      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="qw2" />
				<label for="qw2"><span class='greenDot'></span><span class='clock'></span>17:55-18:30</label>
        <h3> Session 11: Body and Embodiment in Dance Performance</h3>
        <h4> Youhong Peng and Atau Tanaka</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. In this paper, we describe the modalities of the body being interpreted and utilized by various practitioners including, choreographers, artists, and architects through time. The definition of the body has been extended by the concepts of other disciplines, such as philosophy, where some define it as decided by the potential of its actions. Inspired by these philosophical ideas of the body, Skin-awareness, an interactive dance performance, was developed to explore and experiment with the body as a self-aware entity, embodying and interacting with artefacts and an immersive environment. The technical and choreographic design are introduced, followed by a discussion of the composition of the work.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="aq" />
				<label for="aq"><span class='greenDot'></span><span
            class='clock'></span>18:00-18:30</label>
        <h3> Session 12A: com//ense </h3>
        <h4> Performance - Installation </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, Gallery</a></h4>
				<article>
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="aq1" />
				<label for="aq1"><span class='greenDot'></span><span
            class='clock'></span>18:00-18:20</label>
        <h3> Session 12B: DATURA </h3>
          <h4> Kymatocarpa - the Ephemeral Performance Garden </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, Gallery</a></h4>
				<article>
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="aq3" />
				<label for="aq3"><span class='greenDot'></span><span class='clock'></span>18:15-19:00</label>
        <h3> Session 13A: Embodying Notation: Scoring Movement in Augmented Reality </h3>
        <h4> Hannah Kosstrin and Chris Summers</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. How can we capture dance? Once captured, how can we analyze dance movement? Further, how can other dancers embody this captured movement? These are some of the questions we seek to answer with our augmented reality (AR) dance notating tool LabanLens. LabanLens is an application for the Microsoft mixed-reality HoloLens headset. It uses two forms of dance notation: Labanotation, an internationally recognized system for documenting and analyzing dance through dance “scores,” or written representations of movement similar to music notation; and Motif scoring, which describes a dance’s general characteristics and can also be used to generate new dances and analyze existing ones. LabanLens projects new or existing movement notation scores into the user’s field of view, which enables the user to perform them in an immersive videogame-style environment. LabanLens engages innovative digital practices responding to diverse intelligences and abilities to immerse students in a digital-kinesthetic experience with dance analysis. It introduces a modality to re-envision how we teach and employ Laban-based scoring tools, and an immersive environment for composing dances, analyzing them, and understanding historical embodiments. The AR experience engenders studentsʼ articulate analysis of dances, which deepens their kinesthetic-empathetic engagement with each other and their world. LabanLens expands existing dance scoring methodsʼ capacities to advance knowledge for accessing and analyzing new and existing dances, with broader applications for motion analysis in the athletics and health fields. It has additional applications in diverse educational environments.
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="dfg2" />
				<label for="dfg2"><span class='greenDot'></span><span class='clock'></span>18:15-19:15</label>
        <h3> Session 13B: Skeleton Conductor: an interactive real time, movement-based VR experience</h3>
        <h4> Hanna Pajala-Assefa and Cumhur Erkut</h4>
        <h4><a href='javascript:goto("is")'><span class='pin'></span> Matthews Center, iStage </a></h4>
				<article>
          ABSTRACT. Skeleton Conductor (SC) is a cross-disciplinary research and development project that aims towards a new immersive and interactive VR experience, which positions the perceiver and one’s body as an active agent within the virtual realm while creating and interacting with the displayed sensorial input in an head mounted display (HMD). We propose to share the current state of the Skeleton Conductor project and reflect to the many research and technology development-geared questions this project has brought us to. The aim of the practice work is to present preliminary results from our user experience target groups and to reflect upon some of the key research areas which aim to explore the significance of embodied experience. We plan to further define the modes and characteristics of full body real-time interactivity within this particular virtual environment. In practical terms this would mean setting up volumetric capturing equipment and a VR device (Vive, Oculus) and to allow a participants to enter the SC experience. while other participants watch this experience from a large monitor or video projector. With this practice work, we hope to enable knowledge exchange and feedback from our peers, artists and other researchers.
				</article>
			</div>


      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='greenDot'></span><span
            class='clock'></span>12:00-13:00</label>
				<div class='article'>
            Session 14: KHONG KHRO 
            <br><a href='javascript:goto("sfb")'><span
            class='pin'></span> Stauffer B, Gallery </a>
				</div>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="sx" />
				<label for="sx"><span class='blueDot'></span><span class='clock'></span>19:00-19:20</label>
        <h3> Performance Proposal by DATURA: – Kymatocarpa - the Ephemeral Performance Garden</h3>
        <h4> John Mitchell and Halley Willcox </h4>
        <h4><a href='javascript:goto("fac")'><span class='pin'></span> Fine Arts Center, FAC 122 </a></h4>
				<article>
          Datura is proposing to perform a 12 to 20 - minute work titled Kymatocarpa – The Ephemeral Performance Garden.    DATURA is an interdisciplinary performance ensemble formed to develop a multidisciplinary approach to improvisational performance and explore the boundaries and cross overs through the use of digital and analog media and processes in live performance settings. The current lineup includes five dancers, three composer/musicians, and one visual artist/musician, who are creating work at a professional level.   The ensemble works from an event-based score devised by the group. The musicians perform using a combination of analog patch synthesizers, laptop digital synthesis, processed woodwinds, and home-made, amplified percussion instruments. Some dancers wear analog accelerometers that connect to a digitally-modeled Moog synthesizer tuned to 31-tones per octave. Other dancers wear digital accelerometers that feed into an analog patch synthesizer. In this way, during certain parts of the piece the dancers are providing all of the sound as well as the movement. Projected images used in the performance are processed live using input from the sensors as well as a combination of live digital and analog processing.
          <br>
            DATURA Kymatocarpa has the largest flowers of any Datura
          species, the flowers open for only one night and wither the
          next day. The Ephemeral Performance Garden will explore the
          impermanence and beauty of the fleeting elements that exist in
          our lives. Relationships, work, family, living spaces – often
          the things that we most take for granted – are beautiful one
          day (one night) and are gone the next.   For this work we will
          create a Performance Garden in the performance space. This
          garden will be guided, as the viewer experiences a
          walk-through to view and become immersed in the performance.
          The participants will be led, in groups, on a tour of the
          performance space, experiencing the emergent elements of the
          performance piece as they traverse the space.   This is a
          practice work. It is a performance that can be easily
          accommodated by the Nelson Fine Arts Center room 122. We will
          provide our own technology, but will require basic lighting,
          live sound reinforcement in QUAD, and video projection. A
          floor suitable for dance is also needed.    This is a new work
          that has not yet been documented.  video of previous work:
          <a href='https://youtu.be/FFQIGOL5Mxg'>https://youtu.be/FFQIGOL5Mxg</a>  video of sensor rehearsal for
          Kymatocarpa 2-24-2019: <a href='https://youtu.be/fFXiEdsqMq8'>https://youtu.be/fFXiEdsqMq8</a>   website
          with complete list of performances and workshops: <a
            href='datura-phx.org'>datura-phx.org</a>

				</article>
			</div>




      <div class='hr'></div>
      <br><br><br><br>

    <h2 class='tcenter fwnormal' id='d3wp'>Day 3</h2>
    <h3 class='tcenter fwnormal'>Sat, October 12th</h3>
    <h4 class='tcenter legend' style='font-size:90%'>
      <span class='greenDot'></span> Performance
      <span class='yellowDot'></span> Breaks
      <span class='blueDot'></span> Presention
    </h4>

			<div class='pcard'>
        <label><span class='blueDot'></span><span class='clock'></span>12:00-16:30</label>
				<div class='article'>
            REGISTRATION OPEN
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d340" />
				<label for="d340"><span class='blueDot'></span><span
            class='clock'></span>09:00-09:15</label>
        <h3> Force & Motion: Conducting to the Click</h3>
        <h4> Richard Polfreman, Benjamin Oliver, Cheryl Metcalf and Daniel Halford</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. We present preliminary results from an on-going project at the University of Southampton that aims to develop protocols for motion capture with music conducting. These protocols will facilitate the study of conducting gestures, provide a high-quality open-access data set using professional conductors and provide a platform for developing machine learning for conductor following systems. In this paper we explore the potential use of force-plate data to track conductors’ beats as a non-intrusive method for conductor following. Three conductors were captured directing the same piece of music and we analysed a section of the piece where the conductors are working with a click-track to ensure the intended beats share the same timing and there is an inherent ground truth. We then examined the data from force plate and high-end optical marker tracking, against observer beat tapping and click audio to determine whether force plate data could serve as a useful analogue in conductor following. The results suggest that with simple analysis of the data, beats can be extracted with comparable timing accuracy to optical marker tracking.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d341" />
				<label for="d341"><span class='blueDot'></span><span
            class='clock'></span>09:15-09:30</label>
        <h3> Imagery and metaphors: from movement practices to digital and immersive environments</h3>
        <h4> Marina Stergiou, Katerina El Raheb and Yannis Ioannidis</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
            ABSTRACT. Imagery is a commonly used practice that is applied in areas such as sports, rehabilitation, therapy and dance. Especially in dance, students of all ages are encouraged by their teachers to use imagery to improve their performance or clearly understand the form and quality of a movement. Mental imagery aims to stimulate thinking with the body, mostly with metaphoric pictures, that usually trigger the sense of sight. These visual or kinesthetic images may include handling imaginary objects, imagining being in particular environments and many other possible imaginary bodily states and shapes. Nowadays motion sensing, augmented and virtual reality technologies offer powerful tools for implementing interactive and real-time visualizations ,merging the two worlds of mental imagery and immersive technology into a new range of opportunities. In this work we raise the question of how design and transition of certain types of imagery into digital experiences might assist dance training. Within an embodied interactive experience, through visual or other modalities a mental imagery metaphor can be transformed into a visual or other kind of representation consisting an effective and/or creative feedback. In this work, first, we examine the existing imagery approaches in movement practices and we discuss their characteristics. Secondly, we study the existing applications that are proposed by researchers in the field of interactive dance and are categorize them in terms of the modalities that they use and the type of metaphors that they are related to. Based on existing literature on augmented performances and reported metaphors, we propose a practical map for implementing self practice tools, reflection tools and learning environments.
				</article>
			</div>

      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d350" />
				<label for="d350"><span class='blueDot'></span><span
            class='clock'></span>09:30-09:55</label>
        <h3> K-Multiscope: Combining Multiple Kinect Sensors into a Common 3D Coordinate System</h3>
        <h4> Kyle Stewart, Tobias Kohn and Bill Manaris</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
ABSTRACT. We present a method for combining data from multiple Kinect motion-capture sensors into a common coordinate system. Kinect sensors offer a cheaper, potentially less accurate alternative for full-body motion tracking. By incorporating multiple sensors into a multiscopic system, we address potential accuracy and recognition flaws caused by individual sensor conditions, such as occlusions and space limitations, and increase the overall accuracy of skeletal data tracking. We merge data from multiple Kinects using a custom calibration algorithm, called K-Multiscope.  K-Multiscope generates an affine transform for each of the available sensors, and thus combines their data into a common 3D coordinate system. We have incorporated this algorithm into Kuatro, a skeletal data pipeline designed earlier to simplify live motion capture for use in music interaction experiences and installations.  In closing, we present Liminal Space, a live duet performance for cello and dance, which utilizes the Kuatro system to transform dance movements into music.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d351" />
				<label for="d351"><span class='blueDot'></span><span
            class='clock'></span>09:55-10:10</label>
        <h3> The Mirrored Body: Sensation, Agency and Expression in a Video Processed World</h3>
        <h4> Todd Winkler</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. Real-time video processing in multimedia performances can create a video double that recalibrates understandings of both on screen and physical bodies. This mirrored bodily exchange affords new experiences of agency, sensation and expression in the context of theatrical performances. Careful attention is paid to creating a life- sized mirror image that places the processed body in close relationship with the living body. These video processing techniques find their purpose as characters in storytelling for theatre and dance.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d352" />
				<label for="d352"><span class='blueDot'></span><span
            class='clock'></span>10:10-10:25</label>
        <h3> First Steps in Dance Data Science: Educational Design</h3>
        <h4> Yoav Bergner, Shiri Mund, Ofer Chen and William Payne</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. We report results of a design-research effort to develop a culturally-relevant educational experience that can engage high school dancers in statistics and data science. In partnership with a local high school and members of its step team, we explore quantitative analysis of both visual and acoustic data captured from student dance. We describe prototype visualizations and interactive applications for evaluating pose precision, tempo, and timbre. With educational goals in mind, we have constrained our design to using only interpretable features and simple, accessible algorithms.
				</article>
			</div>




      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d353" />
				<label for="d353"><span class='blueDot'></span><span
            class='clock'></span>10:25-10:30</label>
        <h3> Information Augmentation for Human Activity Recognition and Fall Detection using Empirical Mode Decomposition on Smartphone Data</h3>
        <h4> Selçuk Sezer and Elif Surer</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. In this paper, we propose a novel design to reduce the number of sensors used in activity recognition and fall detection by using empirical mode decomposition (EMD) along with gravity filtering so as to untangle the useful information gathered from a single sensor, i.e. accelerometer. We focus on reducing the number of sensors utilized by augmenting the information obtained from accelerometer only given that the accelerometer is the most common and easy to access sensor on smartphones. To do so, one gravity component and three intrinsic mode functions (IMFs) are extracted from the accelerometer signal. In order to assess how informative each component is, the raw components are directly used for classification, i.e. without hand-crafting statistical features. The extracted signal components are then individually fed into parallelized random forest (RF) classifiers. The proposed design is evaluated on the publicly available MobiAct dataset. The results show that by only using accelerometer data within the proposed scheme, it is possible to reach the performance of two sensors (accelerometer and gyroscope) used in a conventional manner. This study provides an efficient and convenient-to-use solution for the smartphone applications in human activity recognition domain.
				</article>
			</div>

      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='yellowDot'></span><span
            class='cup'></span>10:50-11:00</label>
				<div class='article'>
            COFFEE BREAK
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>

      <div class='hr'></div>

			<div class='pcard'>
   <!--
				<input type="checkbox" id="" />
    -->
				<label for=""><span class='blueDot'></span><span
            class='clock'></span>15:00-16:15</label>
          <h3> Session 17: Keynote</h3>
          <h4> Sylvain Moreno</h4>
          <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
              Stauffer B, B 125</a></h4>
          <h4><a href='javascript:gotoA(2)'><span class='bio'></span> Presenter Bio</a></h4>
   <!--
          <article>
          </article>
    -->
			</div>



      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='yellowDot'></span><span
            class='cup'></span>12:15-13:30</label>
				<div class='article'>
            LUNCH BREAK
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>

      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='greenDot'></span><span
            class='clock'></span>12:00-13:30</label>
				<div class='article'>
            Session 7A: Poster Session #1, 6 posters
            <br><a href='javascript:goto("sfb")'><span
                  class='pin'></span> Stauffer B, B 123 </a>
				</div>
			</div>


      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='greenDot'></span><span
            class='clock'></span>12:00-13:00</label>
				<div class='article'>
            Session 7B: KHONG KHRO 
            <br><a href='javascript:goto("sfb")'><span
            class='pin'></span> Stauffer B, Gallery </a>
				</div>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d220" />
				<label for="d220"><span class='blueDot'></span><span
            class='clock'></span>12:00-12:30</label>
        <h3> Session 7C: Intimidatrix</h3>
        <h4> Kristina Warren</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, Gallery </a></h4>
				<article>
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
   <!--
				<input type="checkbox" id="" />
    -->
				<label for=""><span class='blueDot'></span><span
            class='clock'></span>13:30-14:45</label>
          <h3> Session 8: Keynote</h3>
          <h4> Maaike Bleeker</h4>
          <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
              Stauffer B, B125</a></h4>
          <h4><a href='javascript:gotoA(1)'><span class='bio'></span> Presenter Bio</a></h4>
   <!--
          <article>
          </article>
    -->
			</div>



      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='yellowDot'></span><span
            class='cup'></span>14:45-15:00</label>
				<div class='article'>
            COFFEE BREAK
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d240" />
				<label for="d240"><span class='blueDot'></span><span
            class='clock'></span>15:00-15:25</label>
        <h3> Toward Expressive Multi-Platform Teleoperation: Laban-Inspired Concurrent Operation of Multiple Joints on the Rethink Robotics Baxter Robot in Static and Dynamic Tasks</h3>
        <h4> Yichen Zhou, Maxwell Asselmeier and Amy LaViers</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. Human motion calls upon embodied strategies, which can be difficult to replicate in teleoperation architectures. This paper presents a teleoperation method that centers around the Space component of Laban Movement Analysis and may improve the dynamic complexity of teleoperation commands, allowing a trained user to command multiple joint angles at one time via a large database of stored poses, which are indexed by Space parameters. In this paper, this method is compared to a benchmark method, utilizing a joint-by-joint manner of control on a Rethink Robotics Baxter with compliant limbs using a Microsoft Xbox controller. Across four tasks with a trained operator, analysis of the number of active joints at a given point in time and time to completion emphasize the utility that comes with the proposed method. In particular, for the two presented static tasks, the average number of joint angles moving at one time improves and completion times reduce for the proposed method. Plots of behavior show additional qualitative differences in operator strategies and resulting motion, which are also discussed. Future work will extend this initial demonstration to more formal trials with multiple operators. This method may help achieve more fluid, continuous, and improvised motion in teleoperation of robots via gamepads as are currently used in disaster response platforms.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d242" />
				<label for="d242"><span class='blueDot'></span><span
            class='clock'></span>15:25-15:50</label>
        <h3> Tonight We Improvise! Real-time tracking for human-robot improvisational dance</h3>
        <h4> Elizabeth Jochum and Jeroen Derks</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. One challenge in robotics is to develop motion planning tools that enable mobile robots to move safely and predictably in public spaces with people. Navigating public spaces requires a high degree of information about context and environment, which can be partly identified by understanding how people move. Motivated by research in dance and robotics, we developed an exploratory study of improvisational movement for dance performance. Dance forms have recognizable styles and specific interaction patterns (turn-taking, tempo changes, etc.) that reveal important information about behavior and context. Following extensive iterations with expert dancers, we developed a sequence of basic motion algorithms based on improvisation exercises to generate three unique, original performances between a robot and human performers trained in various dance styles. We developed a novel method for tracking dancers in real time using inputs to generate choreography for non-anthropomorphic robots. Although the motion algorithms were identical, the individual dancers generated vastly different performances and elicited unexpected motions and choreographies from the robot. We summarize our study and identify some challenges of devising performances between robots and humans, and outline future work to experiment with more advanced algorithms.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d245" />
				<label for="d245"><span class='blueDot'></span><span
            class='clock'></span>15:50-06:05</label>
        <h3> Embodied Intention: Robot Spinal Initiation to Indicate Directionality</h3>
        <h4> Greg Corness and Kristin Carlson </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. This paper explores how a spine-inspired robot can be designed to provide embodied cues to performers, in order to indicate its intention to change direction through its shifting of weight. Our prior work has explored the importance of utilizing body-level cues such as breath and center of gravity to provide more intuitive information about how a robot, media, or physical agent can act in collaboration with human performers. We detail the design and implemented process of the spinal robot and situate this exploration in a body of work on embodied interaction.
				</article>
			</div>

      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='yellowDot'></span><span
            class='cup'></span>06:05-06:15</label>
				<div class='article'>
            COFFEE BREAK
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="xcv" />
				<label for="xcv"><span class='greenDot'></span><span class='clock'></span>16:15-17:45</label>
        <h3> PANEL -- Generative tension in cross-disciplinary collaboration: Call for provocations and panelists at MOCO 2019</h3>
        <h4> John MacCallum, Teoma Naccarato and Jessica Rajko</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. We pose a question of great significance to the MOCO community, that is: what aspects of your practice/research are invisible to your collaborators? We seek responses from individuals and teams engaged in cross-disciplinary research and collaboration spanning the broad array of practices implicated in the field of ‘movement and computing’. Responses may be in the form of a succinct online provocation (in image, sound, writing, video, or other), and may be submitted independently or collectively. In addition, a group of panelists will be invited to convene for a public discussion at MOCO 2019, drawing on the body of provocations to address the notion of generative tension in cross-disciplinary collaboration. The intention of this panel is to draw out and mobilize critical differences between the motives and methods of various disciplinary communities as a source of mutual inspiration and innovation.
				</article>
			</div>




      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="qw2" />
				<label for="qw2"><span class='greenDot'></span><span class='clock'></span>17:55-18:30</label>
        <h3> Session 11: Body and Embodiment in Dance Performance</h3>
        <h4> Youhong Peng and Atau Tanaka</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. In this paper, we describe the modalities of the body being interpreted and utilized by various practitioners including, choreographers, artists, and architects through time. The definition of the body has been extended by the concepts of other disciplines, such as philosophy, where some define it as decided by the potential of its actions. Inspired by these philosophical ideas of the body, Skin-awareness, an interactive dance performance, was developed to explore and experiment with the body as a self-aware entity, embodying and interacting with artefacts and an immersive environment. The technical and choreographic design are introduced, followed by a discussion of the composition of the work.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="aq" />
				<label for="aq"><span class='greenDot'></span><span
            class='clock'></span>18:00-18:30</label>
        <h3> Session 12A: com//ense </h3>
        <h4> Performance - Installation </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, Gallery</a></h4>
				<article>
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="aq1" />
				<label for="aq1"><span class='greenDot'></span><span
            class='clock'></span>18:00-18:20</label>
        <h3> Session 12B: DATURA </h3>
          <h4> Kymatocarpa - the Ephemeral Performance Garden </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, Gallery</a></h4>
				<article>
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="aq3" />
				<label for="aq3"><span class='greenDot'></span><span class='clock'></span>18:15-19:00</label>
        <h3> Session 13A: Embodying Notation: Scoring Movement in Augmented Reality </h3>
        <h4> Hannah Kosstrin and Chris Summers</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. How can we capture dance? Once captured, how can we analyze dance movement? Further, how can other dancers embody this captured movement? These are some of the questions we seek to answer with our augmented reality (AR) dance notating tool LabanLens. LabanLens is an application for the Microsoft mixed-reality HoloLens headset. It uses two forms of dance notation: Labanotation, an internationally recognized system for documenting and analyzing dance through dance “scores,” or written representations of movement similar to music notation; and Motif scoring, which describes a dance’s general characteristics and can also be used to generate new dances and analyze existing ones. LabanLens projects new or existing movement notation scores into the user’s field of view, which enables the user to perform them in an immersive videogame-style environment. LabanLens engages innovative digital practices responding to diverse intelligences and abilities to immerse students in a digital-kinesthetic experience with dance analysis. It introduces a modality to re-envision how we teach and employ Laban-based scoring tools, and an immersive environment for composing dances, analyzing them, and understanding historical embodiments. The AR experience engenders studentsʼ articulate analysis of dances, which deepens their kinesthetic-empathetic engagement with each other and their world. LabanLens expands existing dance scoring methodsʼ capacities to advance knowledge for accessing and analyzing new and existing dances, with broader applications for motion analysis in the athletics and health fields. It has additional applications in diverse educational environments.
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="dfg2" />
				<label for="dfg2"><span class='greenDot'></span><span class='clock'></span>18:15-19:15</label>
        <h3> Session 13B: Skeleton Conductor: an interactive real time, movement-based VR experience</h3>
        <h4> Hanna Pajala-Assefa and Cumhur Erkut</h4>
        <h4><a href='javascript:goto("is")'><span class='pin'></span> Matthews Center, iStage </a></h4>
				<article>
          ABSTRACT. Skeleton Conductor (SC) is a cross-disciplinary research and development project that aims towards a new immersive and interactive VR experience, which positions the perceiver and one’s body as an active agent within the virtual realm while creating and interacting with the displayed sensorial input in an head mounted display (HMD). We propose to share the current state of the Skeleton Conductor project and reflect to the many research and technology development-geared questions this project has brought us to. The aim of the practice work is to present preliminary results from our user experience target groups and to reflect upon some of the key research areas which aim to explore the significance of embodied experience. We plan to further define the modes and characteristics of full body real-time interactivity within this particular virtual environment. In practical terms this would mean setting up volumetric capturing equipment and a VR device (Vive, Oculus) and to allow a participants to enter the SC experience. while other participants watch this experience from a large monitor or video projector. With this practice work, we hope to enable knowledge exchange and feedback from our peers, artists and other researchers.
				</article>
			</div>


      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='greenDot'></span><span
            class='clock'></span>12:00-13:00</label>
				<div class='article'>
            Session 14: KHONG KHRO 
            <br><a href='javascript:goto("sfb")'><span
            class='pin'></span> Stauffer B, Gallery </a>
				</div>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="sx" />
				<label for="sx"><span class='blueDot'></span><span class='clock'></span>19:00-19:20</label>
        <h3> Performance Proposal by DATURA: – Kymatocarpa - the Ephemeral Performance Garden</h3>
        <h4> John Mitchell and Halley Willcox </h4>
        <h4><a href='javascript:goto("fac")'><span class='pin'></span> Fine Arts Center, FAC 122 </a></h4>
				<article>
          Datura is proposing to perform a 12 to 20 - minute work titled Kymatocarpa – The Ephemeral Performance Garden.    DATURA is an interdisciplinary performance ensemble formed to develop a multidisciplinary approach to improvisational performance and explore the boundaries and cross overs through the use of digital and analog media and processes in live performance settings. The current lineup includes five dancers, three composer/musicians, and one visual artist/musician, who are creating work at a professional level.   The ensemble works from an event-based score devised by the group. The musicians perform using a combination of analog patch synthesizers, laptop digital synthesis, processed woodwinds, and home-made, amplified percussion instruments. Some dancers wear analog accelerometers that connect to a digitally-modeled Moog synthesizer tuned to 31-tones per octave. Other dancers wear digital accelerometers that feed into an analog patch synthesizer. In this way, during certain parts of the piece the dancers are providing all of the sound as well as the movement. Projected images used in the performance are processed live using input from the sensors as well as a combination of live digital and analog processing.
          <br>
            DATURA Kymatocarpa has the largest flowers of any Datura
          species, the flowers open for only one night and wither the
          next day. The Ephemeral Performance Garden will explore the
          impermanence and beauty of the fleeting elements that exist in
          our lives. Relationships, work, family, living spaces – often
          the things that we most take for granted – are beautiful one
          day (one night) and are gone the next.   For this work we will
          create a Performance Garden in the performance space. This
          garden will be guided, as the viewer experiences a
          walk-through to view and become immersed in the performance.
          The participants will be led, in groups, on a tour of the
          performance space, experiencing the emergent elements of the
          performance piece as they traverse the space.   This is a
          practice work. It is a performance that can be easily
          accommodated by the Nelson Fine Arts Center room 122. We will
          provide our own technology, but will require basic lighting,
          live sound reinforcement in QUAD, and video projection. A
          floor suitable for dance is also needed.    This is a new work
          that has not yet been documented.  video of previous work:
          <a href='https://youtu.be/FFQIGOL5Mxg'>https://youtu.be/FFQIGOL5Mxg</a>  video of sensor rehearsal for
          Kymatocarpa 2-24-2019: <a href='https://youtu.be/fFXiEdsqMq8'>https://youtu.be/fFXiEdsqMq8</a>   website
          with complete list of performances and workshops: <a
            href='datura-phx.org'>datura-phx.org</a>

				</article>
			</div>




      <div class='hr'></div>
      <br><br><br><br>

    <h2 class='tcenter fwnormal' id='d3wp'>Day 3</h2>
    <h3 class='tcenter fwnormal'>Sat, October 12th</h3>
    <h4 class='tcenter legend' style='font-size:90%'>
      <span class='greenDot'></span> Performance
      <span class='yellowDot'></span> Breaks
      <span class='blueDot'></span> Presention
    </h4>

			<div class='pcard'>
        <label><span class='blueDot'></span><span class='clock'></span>12:00-16:30</label>
				<div class='article'>
            REGISTRATION OPEN
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>

      <div class='hr'></div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d340" />
				<label for="d340"><span class='blueDot'></span><span
            class='clock'></span>09:00-09:15</label>
        <h3> Force & Motion: Conducting to the Click</h3>
        <h4> Richard Polfreman, Benjamin Oliver, Cheryl Metcalf and Daniel Halford</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. We present preliminary results from an on-going project at the University of Southampton that aims to develop protocols for motion capture with music conducting. These protocols will facilitate the study of conducting gestures, provide a high-quality open-access data set using professional conductors and provide a platform for developing machine learning for conductor following systems. In this paper we explore the potential use of force-plate data to track conductors’ beats as a non-intrusive method for conductor following. Three conductors were captured directing the same piece of music and we analysed a section of the piece where the conductors are working with a click-track to ensure the intended beats share the same timing and there is an inherent ground truth. We then examined the data from force plate and high-end optical marker tracking, against observer beat tapping and click audio to determine whether force plate data could serve as a useful analogue in conductor following. The results suggest that with simple analysis of the data, beats can be extracted with comparable timing accuracy to optical marker tracking.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d341" />
				<label for="d341"><span class='blueDot'></span><span
            class='clock'></span>09:15-09:30</label>
        <h3> Imagery and metaphors: from movement practices to digital and immersive environments</h3>
        <h4> Marina Stergiou, Katerina El Raheb and Yannis Ioannidis</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
            ABSTRACT. Imagery is a commonly used practice that is applied in areas such as sports, rehabilitation, therapy and dance. Especially in dance, students of all ages are encouraged by their teachers to use imagery to improve their performance or clearly understand the form and quality of a movement. Mental imagery aims to stimulate thinking with the body, mostly with metaphoric pictures, that usually trigger the sense of sight. These visual or kinesthetic images may include handling imaginary objects, imagining being in particular environments and many other possible imaginary bodily states and shapes. Nowadays motion sensing, augmented and virtual reality technologies offer powerful tools for implementing interactive and real-time visualizations ,merging the two worlds of mental imagery and immersive technology into a new range of opportunities. In this work we raise the question of how design and transition of certain types of imagery into digital experiences might assist dance training. Within an embodied interactive experience, through visual or other modalities a mental imagery metaphor can be transformed into a visual or other kind of representation consisting an effective and/or creative feedback. In this work, first, we examine the existing imagery approaches in movement practices and we discuss their characteristics. Secondly, we study the existing applications that are proposed by researchers in the field of interactive dance and are categorize them in terms of the modalities that they use and the type of metaphors that they are related to. Based on existing literature on augmented performances and reported metaphors, we propose a practical map for implementing self practice tools, reflection tools and learning environments.
				</article>
			</div>

      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d350" />
				<label for="d350"><span class='blueDot'></span><span
            class='clock'></span>09:30-09:55</label>
        <h3> K-Multiscope: Combining Multiple Kinect Sensors into a Common 3D Coordinate System</h3>
        <h4> Kyle Stewart, Tobias Kohn and Bill Manaris</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
ABSTRACT. We present a method for combining data from multiple Kinect motion-capture sensors into a common coordinate system. Kinect sensors offer a cheaper, potentially less accurate alternative for full-body motion tracking. By incorporating multiple sensors into a multiscopic system, we address potential accuracy and recognition flaws caused by individual sensor conditions, such as occlusions and space limitations, and increase the overall accuracy of skeletal data tracking. We merge data from multiple Kinects using a custom calibration algorithm, called K-Multiscope.  K-Multiscope generates an affine transform for each of the available sensors, and thus combines their data into a common 3D coordinate system. We have incorporated this algorithm into Kuatro, a skeletal data pipeline designed earlier to simplify live motion capture for use in music interaction experiences and installations.  In closing, we present Liminal Space, a live duet performance for cello and dance, which utilizes the Kuatro system to transform dance movements into music.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d351" />
				<label for="d351"><span class='blueDot'></span><span
            class='clock'></span>09:55-10:10</label>
        <h3> The Mirrored Body: Sensation, Agency and Expression in a Video Processed World</h3>
        <h4> Todd Winkler</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. Real-time video processing in multimedia performances can create a video double that recalibrates understandings of both on screen and physical bodies. This mirrored bodily exchange affords new experiences of agency, sensation and expression in the context of theatrical performances. Careful attention is paid to creating a life- sized mirror image that places the processed body in close relationship with the living body. These video processing techniques find their purpose as characters in storytelling for theatre and dance.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d352" />
				<label for="d352"><span class='blueDot'></span><span
            class='clock'></span>10:10-10:25</label>
        <h3> First Steps in Dance Data Science: Educational Design</h3>
        <h4> Yoav Bergner, Shiri Mund, Ofer Chen and William Payne</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. We report results of a design-research effort to develop a culturally-relevant educational experience that can engage high school dancers in statistics and data science. In partnership with a local high school and members of its step team, we explore quantitative analysis of both visual and acoustic data captured from student dance. We describe prototype visualizations and interactive applications for evaluating pose precision, tempo, and timbre. With educational goals in mind, we have constrained our design to using only interpretable features and simple, accessible algorithms.
				</article>
			</div>




      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d353" />
				<label for="d353"><span class='blueDot'></span><span
            class='clock'></span>10:25-10:30</label>
        <h3> Information Augmentation for Human Activity Recognition and Fall Detection using Empirical Mode Decomposition on Smartphone Data</h3>
        <h4> Selçuk Sezer and Elif Surer</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. In this paper, we propose a novel design to reduce the number of sensors used in activity recognition and fall detection by using empirical mode decomposition (EMD) along with gravity filtering so as to untangle the useful information gathered from a single sensor, i.e. accelerometer. We focus on reducing the number of sensors utilized by augmenting the information obtained from accelerometer only given that the accelerometer is the most common and easy to access sensor on smartphones. To do so, one gravity component and three intrinsic mode functions (IMFs) are extracted from the accelerometer signal. In order to assess how informative each component is, the raw components are directly used for classification, i.e. without hand-crafting statistical features. The extracted signal components are then individually fed into parallelized random forest (RF) classifiers. The proposed design is evaluated on the publicly available MobiAct dataset. The results show that by only using accelerometer data within the proposed scheme, it is possible to reach the performance of two sensors (accelerometer and gyroscope) used in a conventional manner. This study provides an efficient and convenient-to-use solution for the smartphone applications in human activity recognition domain.
				</article>
			</div>

      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='yellowDot'></span><span
            class='cup'></span>10:50-11:00</label>
				<div class='article'>
            COFFEE BREAK
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>

      <div class='hr'></div>

			<div class='pcard'>
   <!--
				<input type="checkbox" id="" />
    -->
				<label for=""><span class='blueDot'></span><span
            class='clock'></span>15:00-16:15</label>
          <h3> Session 17: Keynote</h3>
          <h4> Sylvain Moreno</h4>
          <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
              Stauffer B, B 125</a></h4>
          <h4><a href='javascript:gotoA(2)'><span class='bio'></span> Presenter Bio</a></h4>
   <!--
          <article>
          </article>
    -->
			</div>


      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='greenDot'></span><span
            class='clock'></span>12:15-16:30</label>
				<div class='article'>
            Session 18: Poster Session #2
            <br><a href='javascript:goto("sfb")'><span
                  class='pin'></span> Stauffer B, B 123 </a>
				</div>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d401" />
				<label for="d401"><span class='blueDot'></span><span
            class='clock'></span>13:30-13:55</label>
        <h3> Functional Data Analysis of Rowing Technique Using Motion Capture Data</h3>
        <h4> Artur Becker, Henrik Henrik Herrebrøden, Victor Evaristo González Sánchez, Kristian Nymoen, Carla Maria Dal Sasso Freitas, Jim Torresen and Alexander Refsum Jensenius</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. We present an approach to analyzing the motion capture data of rowers using bivariate functional principal component analysis (bfPCA). The method has been applied on data from six elite rowers rowing on an ergometer. The analyses of the upper and lower body coordination during the rowing cycle revealed significant differences between the rowers, even though the data was normalized to account for differences in body dimensions. We make an argument for the use of bfPCA and other functional data analysis methods for the quantitative evaluation and description of technique in sports.
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d402" />
				<label for="d402"><span class='blueDot'></span><span
            class='clock'></span>13:55-14:20</label>
        <h3> Evaluating movement qualities with visual feedback for real-time motion capture</h3>
        <h4> Aishah Hussain, Camilla Mødekjær, Cumhur Erkut, Sofia Dahl and Nicoline Warming Austad</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. The focus of this paper is to investigate how the design of visual feedback on full body movement affect the quality of the movements. Informed by the theory of embodiment in interaction design and media technology, as well as by the Laban theory of effort, a computer application was implemented in which users are able to project their movements onto two visuals ('Particle' and 'Metal') %virtual characters. We investigated whether the visual designs influenced movers through an experiment where participants were randomly assigned to one of the visuals while performing a set of simple tasks. Qualitative analysis of participants' verbal movement descriptions as well as analysis of quantitative movement features combine several perspectives with respect to describing the differences and the change in the movement qualities. The qualitative data shows clear differences between the groups. The quantitative data indicates that all groups move differently when visual feedback is provided. Our results contribute to the design effort of visual modality in movement-focused design of extended realities.
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d403" />
				<label for="d403"><span class='blueDot'></span><span
            class='clock'></span>14:20-14:45</label>
        <h3> The Calder Effect - Embodied Knowledge Through Moving Images</h3>
        <h4> Anne Dubos and Jan Schacher</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125 </a></h4>
				<article>
          ABSTRACT. How can a contemporary technological apparatus reproduce the space of a gesture that is 40'000 years old? Through an iconic (or iconological) process, the project `Les Mains Négatives' is an attempt to combine intensive archaeological, anthropological, iconological, phenomenological research on rock art painting, using technological tools and re-mediation processes from media arts. Focused on a teleological approach, our research addresses the means of communication and interaction: What is happening to one's consciousness when the body moves? Considering that rock art and cave ornamentations are not simple pictorial art, but a place for technical transmission, our aim is to show how rock art has been a medium for teaching via the medium of performance. The `mise-en-oeuvre' of our apparatus generates an artistic object, whose primary goal is knowledge production through aesthetic experience.
				</article>
			</div>


      <div class='hr'></div>
      <div class='pcard'>
        <label><span class='yellowDot'></span><span
            class='cup'></span>14:45-15:00</label>
				<div class='article'>
            COFFEE BREAK
            <br><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a>
				</div>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<label for=""><span class='greenDot'></span><span
            class='clock'></span>12:30-16:30</label>
        <h3> Session 21: MOCO 2020</h3>
        <h4> Discussion </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span>
            Stauffer B, B 125</a></h4>
				<article>
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d500" />
				<label for="d500"><span class='greenDot'></span><span
            class='clock'></span>16:30-17:00</label>
        <h3> Inverted Narratives(IN): Design Through Practice</h3>
        <h4> Christiana Rose</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d501" />
				<label for="d501"><span class='greenDot'></span><span
            class='clock'></span>16:30-17:00</label>
        <h3> Mubone: An Augmented Trombone and Movement-Based Granular Synthesizer </h3>
        <h4> Travis J. West and Kalun Leung</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. The mubone is an incremental evolution of the slide trombone based on a system of electronic augmentations which allow the spatial orientation of the instrument to be incorporated into the control of algorithmic sound and music synthesis. We present our conceptual design for this new instrument, detail its practical implementation with a spatial granular synthesizer, and discuss initial creative uses.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d503" />
				<label for="d503"><span class='greenDot'></span><span
            class='clock'></span>16:30-17:15</label>
        <h3> Session 22C: A Machine</h3>
        <h4> Amy LaViers</h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
          ABSTRACT. “A Machine” is an interactive dance performance exploring metaphors about machines and movement. Participating audience members are seated inside a gridded space on cushions numbered in a base 16 numbering system (hexidecimal), which references the numbering systems often used in assembly languages where commands correspond to directly to transistor-based hardware. Prior to being seated each of these audience members have provided their phones and phone numbers, which are used as interactive elements in the performance, controlled by a computer script. Excerpts from academic papers by Alan Turing (computer science) and Catherine Elgin (philosophy of dance) are incorporated into the show alongside live performance and interaction with audience members, who are queried by the performer. The piece evokes the dark, rigidly structured internal world of computers, made visible and experiential through embodiment. Thus, the piece is a metaphor for robots in factories -- and humans in dance studios -- where movement tasks are systematized.

				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d505" />
				<label for="d505"><span class='greenDot'></span><span
            class='clock'></span>17:00-19:00</label>
        <h3> Session 23: Double Agent: the dancer in the machine</h3>
        <h4> Simon Biggs, Sue Hawksley, Samya Bagchi and Mark McDonnell</h4>
        <h4><a href='javascript:goto("fac")'><span class='pin'></span> Fine Arts Center, FAC 122 </a></h4>
				<article>
ABSTRACT. The subtitle of this presentation, 'the dancer in the machine', evokes Gilbert Ryle’s critique of René Descarte’s mind-body dualism as the "ghost in the machine". [1] Ryle argued that Cartesian dualism depends on a model of the body-mind relationship that posits the mind as a 'ghost' within, or 'puppeteer' of, the physical body. Ryle's is an embodied concept of cognition, where agency is considered enacted not from a central control system but as distributed, akin to what Gregory Bateson subsequently described as an "ecology of mind". [2] In the recent artistic project 'Double Agent' the authors have been exploring dual modalities of agency in the moving body. 'Double Agent' [3] employs machine-learning and computational representation of human movement alongside algorithmic interaction with, and responses to, live human movement. Double Agent is an interactive augmented performance environment where people (interactors) physically interact with a virtual 'agent' within a large-scale three-dimensional projection. The 'agent' is an emergent phenomenon determined by the behaviour of numerous small invisible virtual elements that are both drawn to and repelled by the movement of human bodies in the installation space. The 'agent' is formed from the totality of this behaviour as a complex three-dimensional visual structure that is both tensile and fluid. Interaction with the 'agent' encourages exploration by interactors of the system's tensional polarity and the sense of physical extension it allows. Double Agent was developed as a collaboration between artist Simon Biggs, computer scientists Mark McDonnell and Samya Bagchi and dance artists Sue Hawksley and Tammy Arjona. The project incorporates a software agent within the system that has learned how to dance. The title Double Agent evokes the two-fold agency of the work, wherein a computationally generated agent interacts with a live interactor whilst another computationally generated agent simultaneously 'dances' based on what it has learned. Employing over 8 hours of recorded dance data, acquired through the live motion-capture of two dancers improvising within the work, the software agent has learned to improvise dance movements in response to the live actions of interactors. The software agent moves in ways similar to the dancers but also possesses a host of novel moves. This novelty could be considered a form of creative agency emergent from the machine-learning process. Double Agent employs a Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) [4]. LSTM-RNNs allow computational systems to evolve models of complex behaviour in an unsupervised manner, without reference to pre-existing datasets. The system learns by identifying patterns in the data in what could be conceived of as an idealised non-verbal or non-linguistic experiential framework. Such computational systems can acquire the capacity to generate novel data-sets that follow similar patterns; in the case of Double Agent, humanoid movement data replicating similar, but not identical, behaviour as found in the original motion-capture data. In Double Agent we witness the emergence of a software generated co-interactor, that cohabits a virtual installation space with human interactors, contributing to the collective construction and experience of the work. This software agent is not unaware of its immediate environment. The agent monitors the activity of human interactors and conditions its own behaviour in response, as an inverse correlate: the more active the human interactors the less active the software agent, and vice versa. Here the installation, the software, computers, sensors and interactors (both human and computer-generated) function as a contingent assemblage that, from moment to moment and state to state, instantiates itself as a dynamic heterogeneous subject. Double Agent raises questions about the role of agency within complex distributed systems, whether human, machine or hybrid. In Double Agent there is no 'dancer in the machine'. The system as a whole, including the machine and the human, is the dancer.
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d600" />
				<label for="d600"><span class='greenDot'></span><span
            class='clock'></span>17:30-18:00</label>
        <h3> Exploring Embodied Sonic Meditation through time and space</h3>
        <h4> Jiayue Cecilia Wu, Lauren Hayes and John Robert Ferguson</h4>
        <h4><a href='javascript:goto("is")'><span class='pin'></span> Matthews Center, iStage </a></h4>
				<article>
          ABSTRACT. This improvisation network music will be performed by four computer musicians from different countries and cultural backgrounds using body sensing and network concert technology. Using customized body motion tracking digital music instruments and Jacktrip open-source, low-latency network technology, an embodied sonic meditation experience will be realized, connecting Berlin, Brisbane, and Arizona State University's musicians and audience.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d601" />
				<label for="d601"><span class='greenDot'></span><span
            class='clock'></span>18:00-18:30</label>
        <h3> A Shared Gesture and Positioning System for Smart Environments</h3>
        <h4> Chris Ziegler, Tejaswi Gowda and Brandon Mechtley</h4>
        <h4><a href='javascript:goto("is")'><span class='pin'></span> Matthews Center, iStage </a></h4>
				<article>
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d602" />
				<label for="d602"><span class='greenDot'></span><span
            class='clock'></span>18:30-19:00</label>
        <h3> Augmented Violin Performance: A Model-Free Personal Instrument</h3>
        <h4> Seth Thorn</h4>
        <h4><a href='javascript:goto("is")'><span class='pin'></span> Matthews Center, iStage </a></h4>
				<article>
          ABSTRACT. My augmented violin is a personal instrument consisting of an acoustic violin, real-time signal processing, a custom sensor glove, and a violin shoulder rest embedded with voice coils for haptic feedback. Feature construction in software is not model-based but experimental, continuously refined by trial and error. My approach to movement analysis is not based on classifying formalized styles of bowing but on continuous tracking of bowing movement and variation. These techniques are co- developed with novel sonification methods ad libitum, conditioning the performance space without schematizing possible gestures a priori. Furthermore, ongoing development of the instrument, which includes novel haptic feedback elements, effectively symmetrizes sensory feedforward and feedback paths—the enactive loop between action and perception—yielding refined instrumental dynamics, yet the signal processing decisions entail no claim to universal or scientific validity. I improvise with this instrument in live performance, unfurling its possibilities and resingularizing my own technique(s).
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d603" />
				<label for="d603"><span class='greenDot'></span><span
            class='clock'></span>19:00-19:30</label>
        <h3> Body, Full of Time</h3>
        <h4> Scotty Hardwig and Zach Duer</h4>
        <h4><a href='javascript:goto("is")'><span class='pin'></span> Matthews Center, iStage </a></h4>
				<article>
          Body, Full of Time is a solo choreographic work performed and
          created by dance media artist Scotty Hardwig in collaboration
          with visual artist Zach Duer. Using motion capture,
          projection, and interactive avatar designs, the work presents
          a chimeric vision of the human body fragmented in the cyber
          age, examining the relationship between physical and digital
          versions of self. The dance emerges in the space between the
          human and the virtual, with the body both as active sensor and
          passive recipient to technological forces. In choreography,
          stage design, and sound composition, the work draws upon old
          and new ways of making, melding ancient ways of creating and
          dancing with more contemporary currents in digital
          culture.<br>

This performance integrates inertial motion capture technology with
custom software to freeze, record, and playback portions of a controlled
avatar linked to the movement performer. In this way, the choreography
is re-coded in digital space so that two simultaneous performances are
happening: the movements of the live body alongside the “digital
choreography” of the avatar and animations. This is a hybrid performance
work that draws together the visual languages of dance, choreography,
stage design, sound, visual art, 3D and 2D animation techniques, and
contemporary digital aesthetics. It is in this blending of art forms
that we are researching embodied fragmentation and multiplicity in
three-dimensional virtual space. One of the goals is to investigate this
hybridity of form between traditional choreographic arts and the
potentialities provided by digital technology. In form and in content,
the work investigates the relationship between physical and cyber forms
of embodiment.<br>

The work also follows a somewhat classical structure with three movements. In the first movement, we see the human body in a raw form (it's a very physical, highly choreographic section with minimal projections), in the second movement, we see a duet between the performer and an avatar responding and coded to respond to movements in various ways. In the third movement, we see a pacified, passive body being actively "scanned" by projection mapping software. The three movements take us on a surreal journey from the body as active, into a hybrid technological space, and finally into a passive body subsumed by technological forces.
				</article>
			</div>


      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d700" />
				<label for="d700"><span class='greenDot'></span><span
            class='clock'></span>17:30-17:45</label>
        <h3> Session 24B: Terra Lingua</h3>
        <h4> Teal Darkenwald, John Dixon, Emma Hospelhorn and Ben Sutherland</h4>
        <h4><a href='javascript:goto("is")'><span class='pin'></span> Matthews Center, iStage </a></h4>
				<article>
          ABSTRACT. Terra Lingua, or literally “earth language,” is a performance piece for dancers wearing sound-generating motion capture suits, with optional live instrumentation. While the dancer is moving, sound is generated based on streaming data from inertial measurement units placed around joints that track three-dimensional positional and rotational data; this data simultaneously generates a visual avatar that is projected onto a screen, moving in concert with the dancers. Each dancer movement triggers changes in the sonic environment, while sound generated by live musicians comments upon the changes in the sonic texture. As the piece builds, the parameters by which the sound is generated change. For example, changes in cumulative velocity may create a low humming sound bath, limb acceleration may cause a cacophony of text-based sounds, or changes in the height of the center of mass may create changes in pitch from high to low. In the first movement, Morph, the dancer’s movements influence the timbre of the soundscape; in the second movement, Horizon, the dancers ”swim” through the earth and air, creating sound as they disturb the fluttering fields of air and growling fields of earth. In Babel, embodied motion becomes the underpinnings of speech, which color the rest of the work, culminating in a fragmented soliloquy on what it means to be human. Visually as the piece progresses, the avatar becomes disfigured in an effort to prompt the audience to consider the many ways that technology can both enhance life but also serve as a source of distraction. In Terra Lingua choreography and composition are inseparable, as technology allows a human body in motion to generate the music of the dance. This work is contextualized through artistic expression and experimentation at the intersection of the arts and sciences.
				</article>
			</div>



      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="d720" />
				<label for="d720"><span class='greenDot'></span><span
            class='clock'></span>17:45-18:45</label>
        <h3> Session 25: the rite of computing: Dancing a Universal Turing Machine</h3>
        <h4> Sejo Vega-Cebrián</h4>
        <h4><a href='javascript:goto("is")'><span class='pin'></span> Matthews Center, iStage </a></h4>
				<article>
          ABSTRACT. the rite of computing is a dance in which its
          participants collaborate and become a new kind of computer,
          machine, organism. It originates from two questions: What else
          can computation look like (besides a logic of efficiency and
          productivity, based on material, natural, and social
          exploitation)? What if computation was a ceremony, a party, a
          dance? The work is an exploration of computing in terms of
          slowing down, powering off, and “the pleasure in the confusion
          of boundaries”. It is also an exploration of movement in terms
          of group awareness, mutual agreement, and joy.<br>

the rite of computing is a rule-based improvisation influenced by repetition. In it, human and non-human elements embody a reinterpretation of the Universal Turing Machine described by Minsky and also discussed by Feynman. For this implementation, the computer emulates what Turing called circle-free computing machines; these are “[algorithms that] remain in a state of becoming, endlessly modifying the result”. The dance performs a computation that may never reach an end.
				</article>
			</div>




   <div class='hr'></div>

			<div class='pcard'>
				<label for="wr"><span class='yellowDot'></span><span class='clock'></span>19:00-21:00</label>
        <h4><a href='https://goo.gl/maps/iEypNc1QxbbBmFX37' target='_blank'><span class='pin'></span> ASU Art Museum Rooftop </a></h4>
				<div class='article'>
            Gala! Food, Drink, DJs
				</div>
			</div>





  </div>
</div>



<!--
      <div class='hr'></div>

			<div class='pcard'>
				<input type="checkbox" id="" />
				<label for=""><span class='blueDot'></span><span class='clock'></span>:00-:00</label>
        <h3> </h3>
        <h4> </h4>
        <h4><a href='javascript:goto("sfb")'><span class='pin'></span> Stauffer B </a></h4>
				<article>
				</article>
			</div>
      -->

